{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PyTorch utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "# import numpy, scipy and matplotlib for array ops and plots\n",
    "import numpy as np\n",
    "np.random.seed(3333)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "import timeit\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our synthetic data\n",
    "\n",
    "First let's create a dataset of 5000 samples following a Gaussian distribution with a pre-specified covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n_samples sampled from Gaussian distribution\n",
    "n_samples = 5000\n",
    "\n",
    "cov_mat = 0.01 * np.eye(2) + [[0., 0.0], [0.05, 0.]]\n",
    "print(\"real data cov matrix: \\n\", cov_mat)\n",
    "pdf_x = np.random.multivariate_normal(np.zeros(2), cov_mat, n_samples)\n",
    "# print \"pdf shape: \", pdf_x.shape\n",
    "_ = plt.scatter(pdf_x[:, 0], pdf_x[:, 1], edgecolor='none')\n",
    "_ = plt.title('real data distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "### We want to build a GAN that learns a mapping:\n",
    "\n",
    "#### uniform distribution `Z` in [-1, 1] --> Pdata distribution of `5000` samples.\n",
    "\n",
    "### We can do so with the following networks Generator and Discriminator:\n",
    "\n",
    "```\n",
    "Generator (\n",
    "  (fc1): Linear (50 -> 256)\n",
    "  (fc2): Linear (256 -> 256)\n",
    "  (out_fc): Linear (256 -> 2)\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "Discriminator (\n",
    "  (fc1): Linear (2 -> 128)\n",
    "  (fc2): Linear (128 -> 128)\n",
    "  (out_fc): Linear (128 -> 1)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Build a Generator network with PyTorch tools following the definition provided in the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\" This network maps z (withdrawn from prior distribution) to the pdf in train data with dim `out_dim` \"\"\"\n",
    "    def __init__(self, z_dim=100, out_dim=2, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_fc = nn.Linear(hidden_size, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.leaky_relu(self.fc1(x), 0.3)\n",
    "        h2 = F.leaky_relu(self.fc2(h1), 0.3)\n",
    "        h3 = self.out_fc(h2)\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Build a Discriminator network with PyTorch tools following the definition provided in the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" This network classifies its input as either real or fake \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.leaky_relu(self.fc1(x), 0.3)\n",
    "        h2 = F.leaky_relu(self.fc2(h1), 0.3)\n",
    "        h3 = F.sigmoid(self.out_fc(h2))\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        init.orthogonal(m.weight.data, gain=1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fill in the blank of next code, specifying the loss required to perform a real/fake binary classification in the output of D network. Also, make sure you understand why we have 2 optimizers `g_opt` and `d_opt` before proceeding with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "G_LR = 0.0001\n",
    "D_LR = 0.0001\n",
    "Z_DIM = 50\n",
    "BATCH_SIZE = 100\n",
    "N_EPOCHS = 28\n",
    "VIZ_EVERY = 100\n",
    "D_UPDATES = 1\n",
    "\n",
    "# Create both networks\n",
    "g_net = Generator(z_dim=Z_DIM, hidden_size=256)\n",
    "g_net.apply(weights_init)\n",
    "d_net = Discriminator()\n",
    "d_net.apply(weights_init)\n",
    "\n",
    "print(g_net)\n",
    "print(d_net)\n",
    "\n",
    "# Loss is.... Binary Cross Entropy Loss (classification loss)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create the two optimizers\n",
    "g_opt = optim.Adam(g_net.parameters(), lr=G_LR, betas=(0.5, 0.999))\n",
    "d_opt = optim.Adam(d_net.parameters(), lr=D_LR, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Make the update routine of network G (you should have a look at update routine of network D and map it into G, with proper changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store resulting losses out of training\n",
    "d_rl_losses = []\n",
    "d_fk_losses = []\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Pick a big sample from z and project it through G and compare to pdf_x (original data pdf)\n",
    "# this is not data to be trained on, but to check G projections\n",
    "sample_z = np.random.uniform(-1, 1, [n_samples, Z_DIM]).astype(np.float32)\n",
    "# EXERCISE: NOTE THE VOLATILE=TRUE. WHAT IS IT DOING?\n",
    "v_sample_z = Variable(torch.FloatTensor(sample_z), volatile=True)\n",
    "batches_per_epoch = pdf_x.shape[0] / BATCH_SIZE\n",
    "counter = 0\n",
    "curr_epoch = -1\n",
    "batch_timings = []\n",
    "\n",
    "for counter in tqdm(range(int(N_EPOCHS * batches_per_epoch))):\n",
    "    if counter % batches_per_epoch == 0:\n",
    "        # epoch change. First time this if is true, so also init variables.\n",
    "        batch_idx = 0\n",
    "        curr_epoch += 1\n",
    "        # randomize the pdf_x samples\n",
    "        np.random.shuffle(pdf_x)\n",
    "    beg_t = timeit.default_timer()\n",
    "    # sample a batch from prior pdf z\n",
    "    batch_z = torch.FloatTensor(np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM]).astype(np.float32))\n",
    "    batch_z = Variable(batch_z)\n",
    "    # get a batch of samples from gtruth pdf\n",
    "    batch_x_real = torch.FloatTensor(pdf_x[batch_idx:(batch_idx + BATCH_SIZE)])\n",
    "    batch_x_real = Variable(batch_x_real)\n",
    "    d_opt.zero_grad()\n",
    "    g_opt.zero_grad()\n",
    "    # ------------ DISCRIMINATOR TRAINING\n",
    "    # build real label\n",
    "    for d_i in range(D_UPDATES):\n",
    "        d_opt.zero_grad()\n",
    "        labv = Variable(torch.ones(batch_x_real.size(0)))\n",
    "        # (1) REAL D LOSS\n",
    "        d_real = d_net(batch_x_real)\n",
    "        d_real_loss = criterion(d_real, labv)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # (2) FAKE D LOSS\n",
    "        batch_x_fake = g_net(batch_z)\n",
    "        # EXERCISE: NOTE THE DETACH. WHAT IS IT DOING?\n",
    "        d_fake = d_net(batch_x_fake.detach())\n",
    "        # build fake label\n",
    "        labv.data.fill_(0.)\n",
    "        d_fake_loss = criterion(d_fake, labv)\n",
    "        d_fake_loss.backward()\n",
    "        d_opt.step()\n",
    "    \n",
    "    d_loss = d_fake_loss + d_real_loss\n",
    "    # ------------ GENERATOR TRAINING\n",
    "    # build real label\n",
    "    labv.data.fill_(1.)\n",
    "    batch_x_fake = g_net(batch_z)\n",
    "    g_real = d_net(batch_x_fake)\n",
    "    g_real_loss = criterion(g_real, labv)\n",
    "    g_real_loss.backward()\n",
    "    g_opt.step()\n",
    "    \n",
    "    d_fk_losses.append(d_fake_loss.data.numpy())\n",
    "    d_rl_losses.append(d_real_loss.data.numpy())\n",
    "    d_losses.append(d_loss.data.numpy())\n",
    "    g_losses.append(g_real_loss.data.numpy())\n",
    "    \n",
    "    end_t = timeit.default_timer()\n",
    "    batch_timings.append(end_t - beg_t)\n",
    "    \n",
    "    if counter % VIZ_EVERY == 0:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        fake_pred = g_net(v_sample_z).data.numpy()\n",
    "        _ = plt.scatter(sample_z[:, 0], sample_z[:, 1], edgecolor='none', color='orange')\n",
    "        _ = plt.scatter(pdf_x[:, 0], pdf_x[:, 1], edgecolor='none')\n",
    "        _ = plt.scatter(fake_pred[:, 0], fake_pred[:, 1], color='green', edgecolor='none')\n",
    "        plt.show()\n",
    "print(\"Done training for {} epochs! Elapsed time: {} s\".format(N_EPOCHS, np.sum(batch_timings)))\n",
    "print(\"Total amount of iterations done: \", counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the learning curves (let's see the funny behavior of GANs in action!)\n",
    "_= plt.figure(figsize=(15, 10))\n",
    "_ = plt.plot(d_fk_losses, label='D fake loss', linewidth=2.)\n",
    "_ = plt.plot(d_rl_losses, label='D real loss', linewidth=2.)\n",
    "#plt.plot(d_losses, label='D loss')\n",
    "_ = plt.plot(g_losses, label='G loss', linewidth=2.)\n",
    "_ = plt.legend()\n",
    "# NOTE: there is no optimization towards a minima! Instead they are very noisy and paired to each other! \n",
    "# They have to be balanced through the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Sample a batch of `z` samples and make G infer `G(z)`, placing the result in variable `g_pred` to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beg_t = timeit.default_timer()\n",
    "# sample new z vectors from prior\n",
    "batch_z = torch.FloatTensor(np.random.uniform(-1, 1, [BATCH_SIZE * 100, Z_DIM]).astype(np.float32))\n",
    "g_pred = g_net(Variable(batch_z))\n",
    "g_pred = g_pred.data.numpy()\n",
    "end_t = timeit.default_timer()\n",
    "print(\"Inferred {} G samples in {} s\".format(n_samples, end_t - beg_t))\n",
    "_ = plt.figure(figsize=(10, 10))\n",
    "#_ = plt.scatter(sample_z[:, 0], sample_z[:, 1], color='orange', label='Prior z', edgecolor='none')\n",
    "_ = plt.scatter(pdf_x[:, 0], pdf_x[:, 1], label='Real data', edgecolor='none')\n",
    "_ = plt.scatter(g_pred[:, 0], g_pred[:, 1], color='green', label='Generated data', edgecolor='none')\n",
    "_ = plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
