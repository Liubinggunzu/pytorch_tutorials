{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "# load functional definitions for activations like softmax\n",
    "import torch.nn.functional as F\n",
    "# print utilities with json\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "import timeit\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quick intro: The simplicity of running an RNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate an RNN layer with the RNN API from nn module\n",
    "rnn_layer = nn.RNN(input_size=3, hidden_size=3, num_layers=1, batch_first=True)\n",
    "\n",
    "# Make some fake data\n",
    "fake_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 1, 1]]\n",
    "fake_data = torch.FloatTensor(fake_data)\n",
    "print('fake data tensor size: ', fake_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Iterate with your own loop (yesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_t = []\n",
    "h_t_1 = None\n",
    "for x_t in fake_data:\n",
    "    # turn into 3-D shape [batch_size, seq_len, feat_dim=3]\n",
    "    x_t = Variable(x_t).view(1, -1, 3)\n",
    "    h_t, h_t_1 = rnn_layer(x_t, h_t_1)\n",
    "    H_t.append(h_t.data[0, 0, :].numpy())\n",
    "H_t = np.array(H_t)\n",
    "print(H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Structure your data batching, and leave it to the framework (yesss too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t_1 = None\n",
    "x_t = Variable(fake_data).view(1, -1, 3)\n",
    "H_t, h_t_1 = rnn_layer(x_t, h_t_1)\n",
    "print(H_t.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Going Deeper: Character prediction RNN-LSTM\n",
    "\n",
    "#### We'll now proceed to read a text file from our local dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHARS=5\n",
    "with open('aliceinwonderland.txt') as txt_f:\n",
    "    # read the text file\n",
    "    text = txt_f.read().lower()\n",
    "# get sentences on line break (yes, a bit vague, but ok for this)\n",
    "text_sents = list(filter(lambda x: len(x) > 0, text.split('\\n')))\n",
    "print('text corpus length in chars:', len(text))\n",
    "print('total sentences: ', len(text_sents))\n",
    "\n",
    "max_len = max(len(s) for s in text_sents)\n",
    "min_len = min(len(s) for s in text_sents)\n",
    "print('max sentence len: ', max_len)\n",
    "print('min sentence len: ', min_len)\n",
    "\n",
    "print('Example 10 sentences: ')\n",
    "for i, sent in enumerate(text_sents[:10], start=1):\n",
    "    print('{} >> {}'.format(i, sent))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll print the char2idx to grasp how it looks like\n",
    "print(json.dumps(char2idx, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATIC GRAPH PROCEDURE to inject data\n",
    "# taken from Keras example https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "# ----------------------------------------------------------------\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char2idx[char]] = 1\n",
    "    y[i, char2idx[next_chars[i]]] = 1\n",
    "print('Done vectorizing...')\n",
    "print('X data tensor shape: ', X.shape)\n",
    "print('y data tensor shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note how we had to apply a certain maximum sequence length (40), and then we've built a tensor of input chunks of chars X, and a tensor of next chars x[t] named y**\n",
    "\n",
    "### Gimme some PyTorch, please\n",
    "\n",
    "Now we can unleash PyTorch capabilities to define the char RNN very simplistically, and process sequences **with true variable length!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a sentence encoder helper function\n",
    "def encode_sent(sent):\n",
    "    x_seq = []\n",
    "    for ch in sent:\n",
    "        x_seq.append(char2idx[ch])\n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i, sentence in enumerate(text_sents):\n",
    "    # encode each sentence into its char integer code from char2idx dictionary\n",
    "    X.append(encode_sent(sentence))\n",
    "print('Resulting encoded sequences: ', len(X))\n",
    "print('Example 3 encoded sequences: ')\n",
    "for n, sent in enumerate(X[:3], start=1):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charRNN(nn.Module):\n",
    "    def __init__(self, char_vocab_size, emb_size=15,\n",
    "                 rnn_size=128, rnn_layers=1, rnn_dropout=0.):\n",
    "        super(charRNN, self).__init__()\n",
    "        # build character Embedding layer\n",
    "        self.emb = nn.Embedding(char_vocab_size, emb_size)\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.rnn_size = rnn_size\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size=emb_size,\n",
    "                            hidden_size=rnn_size,\n",
    "                            num_layers=rnn_layers,\n",
    "                            dropout=rnn_dropout,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        # FC output layer into num classes (vocab size)\n",
    "        self.fc = nn.Linear(rnn_size, char_vocab_size)\n",
    "    \n",
    "    def forward(self, seq, states=None):\n",
    "        # input tensor is of shape [batch_size, seq_len]\n",
    "        # it is a LongTensor containing an integer idx per char per seq\n",
    "        assert len(seq.size()) == 2, seq.size()\n",
    "        # project seq through embedding layer\n",
    "        emb_ch = self.emb(seq)\n",
    "        # emb_ch ~ [batch_size, seq_len, emb_size]\n",
    "        H_t, states = self.lstm(emb_ch, states)\n",
    "        # H_t ~ [batch_size, seq_len, rnn_size]\n",
    "        # unroll tensor to 2-D to adjust to FC nature\n",
    "        H_t_u = H_t.contiguous().view(-1, H_t.size(-1))\n",
    "        # H_t_u ~ [batch_size x seq_len, rnn_size]\n",
    "        y_t = F.log_softmax(self.fc(H_t_u))\n",
    "        # return output predicted probs and rnn states\n",
    "        return y_t, states\n",
    "\n",
    "    def init_hidden_zero(self, curr_bsz):\n",
    "        return (Variable(torch.zeros(self.rnn_layers, curr_bsz, self.rnn_size)),\n",
    "                Variable(torch.zeros(self.rnn_layers, curr_bsz, self.rnn_size)))\n",
    "        \n",
    "# instance our lstm model\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "print(char_lstm)\n",
    "\n",
    "# HELPER FUNCTION WE'LL USE WE'LL SEE WHERE\n",
    "def repackage_hidden(h):\n",
    "    # https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a sentence decoder function to simplify our future calls to check predictions\n",
    "def decode_pred_sent(pred_sent):\n",
    "    dec_sent = ''\n",
    "    for idx in pred_sent:\n",
    "        dec_sent += idx2char[idx]\n",
    "    return dec_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text prediction function\n",
    "def predict_text_from_seed(model, text_seed='alice was beginning ', num_preds=20):\n",
    "    model.eval()\n",
    "    # begin heating the lstm with some seed text and predict next num_preds chars\n",
    "    enc_seed = encode_sent(text_seed)\n",
    "\n",
    "    # first make the warm up to save into the LSTM states the seed\n",
    "    states = None\n",
    "    for code in enc_seed:\n",
    "        # please note the Volatile keyword: we just want inference, not backprop\n",
    "        code = Variable(torch.LongTensor([code]).view(1, 1), volatile=True)\n",
    "        pred_ch, states = model(code, states)\n",
    "\n",
    "    pred_ch = torch.max(pred_ch, dim=-1)[1]\n",
    "    # |> symbol serves as a simple prompt to check where does it start the prediction\n",
    "    resulting_pred = text_seed + '|>' + idx2char[pred_ch.data[0]]\n",
    "    # Now iterate char by char in a feedback fashion\n",
    "    for n in range(num_preds):\n",
    "        pred_ch, states = model(pred_ch.view(1, 1), states)\n",
    "        pred_ch = torch.max(pred_ch, dim=-1)[1]\n",
    "        pred_idx = pred_ch.data[0]\n",
    "        resulting_pred += idx2char[pred_idx]\n",
    "    return resulting_pred\n",
    "\n",
    "# predict some chars w/ RANDOM weights\n",
    "print('resulting text: ', predict_text_from_seed(char_lstm, 'i found a door ', num_preds=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe how we can iterate with external for loops to the LSTM structure, feeding back the data from time-step to time-step, contrary to TensorFlow methodology.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seqbyseq_epoch(X, curr_epoch, model, opt, log_freq, stateful=False):\n",
    "    # A simple training loop sequence by sequence will serve our purposes\n",
    "    # YES! THAT SIMPLE!!\n",
    "    # specify we are in train mode (this will set Dropout/BN behaviors to train mode)\n",
    "    model.train()\n",
    "    states = model.init_hidden_zero(1)\n",
    "    tr_losses = []\n",
    "    for bidx, x in enumerate(X, start=1):\n",
    "        # clean previous gradients in graph\n",
    "        opt.zero_grad()\n",
    "        # build input x and output y\n",
    "        # output is just a shifted by 1 timestep version of x\n",
    "        y = x[1:]\n",
    "        x = x[:-1]\n",
    "        # format the PyTorch Variables\n",
    "        y = Variable(torch.LongTensor(y)).view(-1) # y ~ [seq_len]\n",
    "        x = Variable(torch.LongTensor(x)).view(1, -1) # x ~ [1, seq_len]\n",
    "        # (1) forward sequence\n",
    "        if not stateful:\n",
    "            pred, _ = model(x, states)\n",
    "        else:\n",
    "            pred, states = model(x, states)\n",
    "            states = repackage_hidden(states)\n",
    "            #raise NotImplementedError('Missing stateful implementation')\n",
    "        \n",
    "        # (2) compute loss: Negative Log Likelihood of correct classes\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        # (3) backprop gradients\n",
    "        loss.backward()\n",
    "        # (4) update parameters\n",
    "        opt.step()\n",
    "        if bidx % log_freq == 0 or bidx >= len(X):\n",
    "            tr_losses.append(loss.data[0])\n",
    "            print('Batch {:4d}/{:4d} (epoch {:3d}) loss {:.3f}'.format(bidx,\n",
    "                                                                       len(X),\n",
    "                                                                       curr_epoch,\n",
    "                                                                       tr_losses[-1]))\n",
    "    return tr_losses\n",
    "            \n",
    "import timeit # time epochs\n",
    "\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.Adam(char_lstm.parameters(), lr=0.001)\n",
    "beg_t = timeit.default_timer()\n",
    "tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    tr_losses += train_seqbyseq_epoch(tr_X, epoch, char_lstm, opt, 400, False)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_text_from_seed(char_lstm, 'she found a door ', num_preds=100))\n",
    "print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Seqbyseq training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(tr_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well look at that learning noise... :( Let's smooth the learning with better updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches_epoch(X, batch_size, curr_epoch, model, opt, log_freq):\n",
    "    # We still need batching with pytorch to smooth our training in the SGD training\n",
    "    # BUT: Each batch can have a variable length in sequence dimension!!\n",
    "    # specify we are in train mode (this will set Dropout/BN behaviors to train mode)\n",
    "    model.train()\n",
    "    \n",
    "    n_batches = int(np.ceil(len(X) / batch_size))\n",
    "    #print('Number of batches with batch_size {}:{}'.format(batch_size, n_batches))\n",
    "    tr_losses = []\n",
    "    for bidx, b_beg in enumerate(range(0, len(X), batch_size), start=1):\n",
    "        left = min(len(X) - b_beg, batch_size)\n",
    "        states = model.init_hidden_zero(left)\n",
    "        # select batch of sequences of outputs and inputs\n",
    "        x_batch = X[b_beg:b_beg + left]\n",
    "        # get max_len and add PADDING to smaller seqs\n",
    "        max_batch_len = max(len(s) for s in x_batch)\n",
    "        min_batch_len = min(len(s) for s in x_batch)\n",
    "        for s_idx in range(len(x_batch)):\n",
    "            x_batch[s_idx] = [0] * (max_batch_len - len(x_batch[s_idx])) + x_batch[s_idx]\n",
    "        x_batch = np.array(x_batch, dtype=np.int64)\n",
    "        y_batch = x_batch[:, 1:]\n",
    "        x_batch = x_batch[:, :-1]\n",
    "        # clean previous gradients in graph\n",
    "        opt.zero_grad()\n",
    "        # format the PyTorch Variables\n",
    "        y = Variable(torch.from_numpy(y_batch).contiguous()).view(-1) # y ~ [batch_size x seq_len]\n",
    "        x = Variable(torch.from_numpy(x_batch).contiguous()).view(left, -1) # x ~ [batch_size, seq_len]\n",
    "        # (1) forward sequence\n",
    "        pred, _ = model(x, states)\n",
    "        \n",
    "        # (2) compute loss: Negative Log Likelihood of correct classes\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        # (3) backprop gradients\n",
    "        loss.backward()\n",
    "        # (4) update parameters\n",
    "        opt.step()\n",
    "        if bidx % log_freq == 0 or bidx >= n_batches:\n",
    "            tr_losses.append(loss.data[0])\n",
    "            print('Batch {:4d}/{:4d} (epoch {:3d}) loss {:.3f}'.format(bidx,\n",
    "                                                                       n_batches,\n",
    "                                                                       curr_epoch,\n",
    "                                                                       tr_losses[-1]))\n",
    "    return tr_losses\n",
    "\n",
    "\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.Adam(char_lstm.parameters(), lr=0.001)\n",
    "beg_t = timeit.default_timer()\n",
    "b_tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    b_tr_losses += train_batches_epoch(tr_X, 10, epoch, char_lstm, opt, 400)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Batched training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(b_tr_losses, label='batched learning')\n",
    "plt.plot(tr_losses, label='sample-wise learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last stage: limiting the max sequence length with statefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_stateful_maxlen_batches_epoch(X, batch_size, max_len, curr_epoch, model, opt, log_freq, clip=0.):\n",
    "    # specify we are in train mode (this will set Dropout/BN behaviors to train mode)\n",
    "    model.train()\n",
    "    # build a super long sequence out of all samples concatenated\n",
    "    X_long = []\n",
    "    for x in X:\n",
    "        X_long += x\n",
    "    # trim to multiple of seqlen x batch_size\n",
    "    X_long = X_long[:batch_size * max_len * (len(X_long) // (batch_size * max_len))]\n",
    "    X_arr = np.array(X_long, dtype=np.int64).reshape((-1, 1))\n",
    "    X_arr = X_arr.reshape((batch_size, -1, 1))\n",
    "    X_arr = np.split(X_arr, X_arr.shape[1] // max_len, axis=1)\n",
    "    X_arr = np.concatenate(X_arr, axis=0)\n",
    "    # print('X_arr shape: ', X_arr.shape)\n",
    "    n_batches = int(np.ceil(X_arr.shape[0] / batch_size))\n",
    "    # print('Number of batches with batch_size {}: {}'.format(batch_size, n_batches))\n",
    "    tr_losses = []\n",
    "    states = model.init_hidden_zero(batch_size)\n",
    "    for bidx, b_beg in enumerate(range(0, X_arr.shape[0], batch_size), start=1):\n",
    "        # select batch of sequences of outputs and inputs\n",
    "        x_batch = X_arr[b_beg:b_beg + batch_size]\n",
    "        x_batch = np.array(x_batch, dtype=np.int64)\n",
    "        y_batch = x_batch[:, 1:]\n",
    "        x_batch = x_batch[:, :-1]\n",
    "        # clean previous gradients in graph\n",
    "        opt.zero_grad()\n",
    "        # format the PyTorch Variables\n",
    "        y = Variable(torch.from_numpy(y_batch).contiguous()).view(-1) # y ~ [batch_size x seq_len]\n",
    "        x = Variable(torch.from_numpy(x_batch).contiguous()).view(batch_size, -1) # x ~ [batch_size, seq_len]\n",
    "        # (1) forward sequence\n",
    "        pred, states = model(x, states)\n",
    "        states = repackage_hidden(states)\n",
    "        # (2) compute loss: Negative Log Likelihood of correct classes\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        # (3) backprop gradients\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "\n",
    "        # (4) update parameters\n",
    "        opt.step()\n",
    "        if bidx % log_freq == 0 or bidx >= n_batches:\n",
    "            tr_losses.append(loss.data[0])\n",
    "            print('Batch {:4d}/{:4d} (epoch {:3d}) loss {:.3f}'.format(bidx,\n",
    "                                                                       n_batches,\n",
    "                                                                       curr_epoch,\n",
    "                                                                       tr_losses[-1]))\n",
    "    return tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.Adam(char_lstm.parameters(), lr=0.001)\n",
    "beg_t = timeit.default_timer()\n",
    "st_tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    st_tr_losses += train_stateful_maxlen_batches_epoch(tr_X, 10, 35, epoch, char_lstm, opt, 400)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Stateful Batched training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(b_tr_losses, label='batched learning')\n",
    "plt.plot(st_tr_losses, label='stateful batched learning')\n",
    "plt.plot(tr_losses, label='sample-wise learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now max_len = 15, shorter to backprop less tsteps\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.RMSprop(char_lstm.parameters(), lr=0.01)\n",
    "beg_t = timeit.default_timer()\n",
    "st_sh_tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    st_sh_tr_losses += train_stateful_maxlen_batches_epoch(tr_X, 10, 15, epoch, char_lstm, opt, 400)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Stateful Batched training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(b_tr_losses, label='batched learning')\n",
    "plt.plot(st_tr_losses, label='stateful-35 batched learning')\n",
    "plt.plot(st_sh_tr_losses, label='stateful-15 batched learning')\n",
    "plt.plot(tr_losses, label='sample-wise learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now max_len = 15, shorter to backprop less tsteps\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.RMSprop(char_lstm.parameters(), lr=0.01)\n",
    "beg_t = timeit.default_timer()\n",
    "st_sh_sh_tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    st_sh_sh_tr_losses += train_stateful_maxlen_batches_epoch(tr_X, 10, 10, epoch, char_lstm, opt, 400)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Stateful Batched training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(b_tr_losses, label='batched learning')\n",
    "plt.plot(st_tr_losses, label='stateful-35 batched learning')\n",
    "plt.plot(st_sh_tr_losses, label='stateful-15 batched learning')\n",
    "plt.plot(st_sh_sh_tr_losses, label='stateful-10 batched learning')\n",
    "plt.plot(tr_losses, label='sample-wise learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now max_len = 15, shorter to backprop less tsteps\n",
    "char_lstm = charRNN(len(char2idx), rnn_size=100, rnn_dropout=0.)\n",
    "NUM_EPOCHS=20\n",
    "LOG_FREQ=400\n",
    "# we can limit the samples in the dataset to speed up training (this is a toy example, remember)\n",
    "max_samples = 100\n",
    "tr_X = X[:max_samples]\n",
    "opt = optim.RMSprop(char_lstm.parameters(), lr=0.01)\n",
    "beg_t = timeit.default_timer()\n",
    "st_cn_tr_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # shuffle sentences\n",
    "    shuffle(tr_X)\n",
    "    st_cn_tr_losses += train_stateful_maxlen_batches_epoch(tr_X, 10, 35, epoch, char_lstm, opt, 400, 0.2)\n",
    "    end_t = timeit.default_timer()\n",
    "    print('Elapsed time for epoch {:3d}: {:.3f} s'.format(epoch, end_t - beg_t))\n",
    "    beg_t = end_t\n",
    "    # see predictions change\n",
    "    print('Epoch {} result {}'.format(epoch, '-' * 30))\n",
    "    print(predict_text_from_seed(char_lstm, text_sents[1], num_preds=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_text_from_seed(char_lstm, 'she found a door ', num_preds=50))\n",
    "print(predict_text_from_seed(char_lstm, 'she went ', num_preds=50))\n",
    "print(predict_text_from_seed(char_lstm, 'when ', num_preds=50))\n",
    "print(predict_text_from_seed(char_lstm, 'a rabbit ', num_preds=50))\n",
    "print(predict_text_from_seed(char_lstm, 'a golden key ', num_preds=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Stateful Batched training curve')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.xlabel('Log tick of freq {}'.format(LOG_FREQ))\n",
    "plt.plot(b_tr_losses, label='batched learning')\n",
    "plt.plot(st_tr_losses, label='stateful-35 batched learning')\n",
    "plt.plot(st_cn_tr_losses, label='stateful-35 cnorm 0.2 batched learning')\n",
    "plt.plot(st_sh_tr_losses, label='stateful-15 batched learning')\n",
    "plt.plot(st_sh_sh_tr_losses, label='stateful-10 batched learning')\n",
    "plt.plot(tr_losses, label='sample-wise learning')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposals to do \n",
    "\n",
    "* Change the dataset into another txt file\n",
    "* Introduce a sequence length filtering in model forward, to **exclude padding positions** in sequence when batching with padding\n",
    "* Implement a noisy initial hidden state with normal distribution ~ N(0, I)\n",
    "* Implement a **learnable initial hidden state** (not just zero vector)\n",
    "* Implement the option to use GRU layers instead of LSTM in the model (BEWARE with cell differences, like states)\n",
    "* Implement the option to use RNN layers instead of LSTM in the model (BEWARE with cell differences, like states)\n",
    "* Compare the **performance of RNN vs GRU vs LSTM**, training for 20 epochs with the desired amount of data\n",
    "* Implement **stateful batched trainer** method (making sample i-th from batch b-th continue in sample i-th from batch (b+1)-th"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
