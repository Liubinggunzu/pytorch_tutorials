{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version:  0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tv = torch.__version__\n",
    "print('Using PyTorch version: ', tv)\n",
    "# check we have PyTorch 0.2.x\n",
    "assert tv[0] == '0' and tv[2] == '2', tv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First things first: The world becomes tensorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions:  3\n",
      "Shape of each dimension:  (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Every deep learning framework is built upon Tensors\n",
    "# These are marvelous multi-dimensional structures\n",
    "# We can create Tensors out of Python lists or NumPy arrays\n",
    "my_list = [0, 1, 2, 3]\n",
    "my_array = np.array(my_list)\n",
    "my_list_T = torch.LongTensor(my_list)\n",
    "my_array_T = torch.LongTensor(my_array)\n",
    "# These are the same, so the assertion will confirm it\n",
    "assert type(my_list_T) == type(my_array_T)\n",
    "\n",
    "# Now we'll create a multi-dimensional array out of a list of lists of lists (3-D)\n",
    "T_3 = [[[0, 1, 2.], [5, 6, 7]], [[0.2, 0.4, 2.2], [4.5, -6, -9]]]\n",
    "T_3 = np.array(T_3)\n",
    "\n",
    "assert T_3.ndim == 3, T_3.ndim\n",
    "print('Number of dimensions: ', T_3.ndim)\n",
    "print('Shape of each dimension: ', T_3.shape)\n",
    "# the dimensions of this NumPy array are [2, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratz for your marvelous Tensors, but now what? \n",
    "Tensors have:\n",
    "1. Info about the data type and the size of each dimension (but NumPy too!)\n",
    "2. the GPU capabilities (NumPy DOES NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([25])\n",
      "W size:  torch.Size([100, 25])\n",
      "b size:  torch.Size([100])\n",
      "y = Wx + b, size:  torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# We can operate with Tensors of course\n",
    "# weights matrix with [inputs x outputs] = [25 x 100]\n",
    "W = torch.randn(100, 25)\n",
    "# bias vector [100]\n",
    "b = torch.zeros(100)\n",
    "# input vector [25]\n",
    "x = torch.randn(25)\n",
    "# Yes, this is a single layer fully connected neural network\n",
    "y = torch.matmul(W, x) + b\n",
    "# y ~ [100] output vector\n",
    "print('x size: ', x.size())\n",
    "print('W size: ', W.size())\n",
    "print('b size: ', b.size())\n",
    "print('y = Wx + b, size: ', y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some PyTorch notation for Tensors properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y size:  torch.Size([100])\n",
      "z size (y reshaped to 10x10):  torch.Size([10, 10])\n",
      "Y size (y unsqueezed in last dim):  torch.Size([100, 1])\n",
      "Y transposed size:  torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "# NumPy --> PyTorch translation\n",
    "# --------------------------------\n",
    "# 1) shape --> size()\n",
    "y.size()\n",
    "print('y size: ', y.size())\n",
    "\n",
    "# 2) reshape() --> view()\n",
    "z = y.view(10, 10)\n",
    "print('z size (y reshaped to 10x10): ', z.size())\n",
    "\n",
    "# 3) expand_dims() --> unsqueeze()\n",
    "Y = y.unsqueeze(-1)\n",
    "print('Y size (y unsqueezed in last dim): ', Y.size())\n",
    "\n",
    "# 4) transpose(0, 1) --> t()\n",
    "Y_t = Y.t()\n",
    "print('Y transposed size: ', Y_t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"magic\" behind AUTOGRAD\n",
    "\n",
    "**Variable:** It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call `.backward()` and have all the gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the `.data` attribute, while the gradient w.r.t. this variable is accumulated into `.grad`[[1]](http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6762 -1.1325 -1.2233  0.6894  0.2096  0.2356 -1.3594 -0.3461 -1.6773  0.4457\n",
      "-0.2565 -0.5017 -0.0967 -2.1807  1.4041 -1.4717  0.8760 -0.2092 -0.7740  3.6363\n",
      " 1.1446 -1.9860  1.1396 -1.9739  0.7423  1.2037  0.4946 -0.1856 -0.4834 -1.3952\n",
      "-0.0324  0.4748  0.0979  1.1420 -0.4775  0.2321 -2.5791 -1.3295 -0.2176  0.5631\n",
      "-0.8356 -0.8526  0.2375  1.7524  1.1189 -0.1390  0.0773 -0.6712 -1.4016  0.8095\n",
      " 1.7392 -0.0452  0.0349  0.6899 -1.5706  0.7190 -1.0788 -0.3919 -1.0044  0.8994\n",
      "-0.1986  0.9016 -0.1839  0.4053 -0.5015 -0.2803  0.6005  2.2463  0.4193  0.0309\n",
      "-0.4738  0.3423  0.3874  0.3733  0.5238 -0.4901  1.0321 -0.4264  1.4701  0.4584\n",
      "-2.6641  1.2440 -1.8874 -0.2299  0.0175 -1.0083 -1.7755 -0.3415  1.7290 -0.7525\n",
      "-1.7420  0.2854  0.2101 -0.9111 -1.4660 -0.6723  0.7580  0.1770  0.3068 -0.0180\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "T = torch.randn(10, 10)\n",
    "# we make the Variable by just wrapping the Tensor with it\n",
    "V = Variable(T)\n",
    "# This is a Variable containing a FloatTensor\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reason to create Variables: the Graph\n",
    "\n",
    "Tensors are nodes in the graph. Edges are the computations relating Tensors (as in TensorFlow). However, the main difference between PyTorch and TensorFlow is: **DYNAMIC GRAPH!**\n",
    "\n",
    "<img src=\"dynamic_graph.gif\" width=\"600px\">\n",
    "\n",
    "[comment]: (Reference_for_the_figure:https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1)\n",
    "\n",
    "The Graph is built operation by operation, thus on runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a graph creation z = sum(x * y)\n",
    "# requires_grad tells the framework we want the gradient wrt to that variable to be computed\n",
    "x = Variable(torch.ones(10), requires_grad=True)\n",
    "y = Variable(torch.ones(10), requires_grad=True)\n",
    "z = x + y\n",
    "out = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "None\n",
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For further reference: http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
