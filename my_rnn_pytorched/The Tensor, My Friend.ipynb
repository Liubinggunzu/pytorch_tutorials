{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tv = torch.__version__\n",
    "print('Using PyTorch version: ', tv)\n",
    "# check we have PyTorch 0.2.x\n",
    "assert tv[0] == '0' and tv[2] == '2', tv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First things first: The world becomes tensorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every deep learning framework is built upon Tensors\n",
    "# These are marvelous multi-dimensional structures\n",
    "# We can create Tensors out of Python lists or NumPy arrays\n",
    "my_list = [0, 1, 2, 3]\n",
    "my_array = np.array(my_list)\n",
    "my_list_T = torch.LongTensor(my_list)\n",
    "my_array_T = torch.LongTensor(my_array)\n",
    "# These are the same, so the assertion will confirm it\n",
    "assert type(my_list_T) == type(my_array_T)\n",
    "\n",
    "# Now we'll create a multi-dimensional array out of a list of lists of lists (3-D)\n",
    "T_3 = [[[0, 1, 2.], [5, 6, 7]], [[0.2, 0.4, 2.2], [4.5, -6, -9]]]\n",
    "T_3 = np.array(T_3)\n",
    "\n",
    "assert T_3.ndim == 3, T_3.ndim\n",
    "print('Number of dimensions: ', T_3.ndim)\n",
    "print('Shape of each dimension: ', T_3.shape)\n",
    "# the dimensions of this NumPy array are [2, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratz for your marvelous Tensors, but now what? \n",
    "Tensors have:\n",
    "1. Info about the data type and the size of each dimension (but NumPy too!)\n",
    "2. the GPU capabilities (NumPy DOES NOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can operate with Tensors of course\n",
    "# weights matrix with [inputs x outputs] = [25 x 100]\n",
    "W = torch.randn(100, 25)\n",
    "# bias vector [100]\n",
    "b = torch.zeros(100)\n",
    "# input vector [25]\n",
    "x = torch.randn(25)\n",
    "# Yes, this is a single layer fully connected neural network\n",
    "y = torch.matmul(W, x) + b\n",
    "# y ~ [100] output vector\n",
    "print('x size: ', x.size())\n",
    "print('W size: ', W.size())\n",
    "print('b size: ', b.size())\n",
    "print('y = Wx + b, size: ', y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some PyTorch notation for Tensors properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy --> PyTorch translation\n",
    "# --------------------------------\n",
    "# 1) shape --> size()\n",
    "y.size()\n",
    "print('y size: ', y.size())\n",
    "\n",
    "# 2) reshape() --> view()\n",
    "z = y.view(10, 10)\n",
    "print('z size (y reshaped to 10x10): ', z.size())\n",
    "\n",
    "# 3) expand_dims() --> unsqueeze()\n",
    "Y = y.unsqueeze(-1)\n",
    "print('Y size (y unsqueezed in last dim): ', Y.size())\n",
    "\n",
    "# 4) transpose(0, 1) --> t()\n",
    "Y_t = Y.t()\n",
    "print('Y transposed size: ', Y_t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"magic\" behind AUTOGRAD\n",
    "\n",
    "**Variable:** It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call `.backward()` and have all the gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the `.data` attribute, while the gradient w.r.t. this variable is accumulated into `.grad`[[1]](http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "T = torch.randn(10, 10)\n",
    "# we make the Variable by just wrapping the Tensor with it\n",
    "V = Variable(T)\n",
    "# This is a Variable containing a FloatTensor\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reason to create Variables: the Graph\n",
    "\n",
    "Tensors are nodes in the graph. Edges are the computations relating Tensors (as in TensorFlow). However, the main difference between PyTorch and TensorFlow is: **DYNAMIC GRAPH!**\n",
    "\n",
    "<img src=\"dynamic_graph.gif\" width=\"600px\">\n",
    "\n",
    "[comment]: (Reference_for_the_figure:https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1)\n",
    "\n",
    "The Graph is built operation by operation, thus on runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a graph creation z = sum(x * y)\n",
    "# requires_grad tells the framework we want the gradient wrt to that variable to be computed\n",
    "x = Variable(torch.ones(10), requires_grad=True)\n",
    "y = Variable(torch.ones(10), requires_grad=True)\n",
    "z = x + y\n",
    "out = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For further reference: http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
